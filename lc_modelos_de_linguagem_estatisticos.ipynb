{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adsLopess/Linguistica_Computacional/blob/main/lc_modelos_de_linguagem_estatisticos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Introduzindo o Processamento de Linguagem Natural (NLP) com Modelo de Linguagem baseado em N-gramas**\n",
        "\n",
        "## **Introdução**\n",
        "\n",
        "**As máquinas estão aprendendo a linguagem humana!** ***Imagine um futuro onde as máquinas compreendem e geram linguagem como humanos***. Essa é a promessa empolgante dos modelos de linguagem, sistemas de inteligência artificial que aprendem a complexidade do mundo das palavras.\n",
        "\n",
        "**Mas como isso funciona?**\n",
        "\n",
        "Exploraremos o conceito de modelos de linguagem e como eles podem ser usados para processar linguagem natural (NLP). Começaremos definindo as etapas para construção de um modelo de linguagem (coleta, pré-processamento, treinamento, etc). Em seguida, mostraremos como os modelos de linguagem podem ser usados para gerar texto automaticamente. Finalmente, abordaremos como os computadores podem calcular as chances de uma palavra ou sequência de palavras ocorrerem em um texto.\n",
        "\n",
        "Para pessoas que cairam de paraquedas nesse mundo, incluímos uma imagem explicativa aprofundar sua compreensão do NLP:\n",
        "\n",
        "<img src=\"https://files.tecnoblog.net/wp-content/uploads/2022/01/o-que-e-processamento-de-linguagem-natural.png\" width=550 height=300>\n",
        "\n"
      ],
      "metadata": {
        "id": "O7uHHk5nkV44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conteúdo que será abordado neste caderno:**\n",
        "\n",
        "**Modelos de Linguagem estatísticos:**\n",
        "* Conceito e construção.\n",
        "* Implementação usando n-gramas.\n",
        "\n",
        "**Geração de texto:**\n",
        "* Como gerar texto automaticamente com modelos de linguagem.\n",
        "* Fornecimento de contexto e especificação do número de palavras.\n",
        "\n",
        "**Cálculo de probabilidades:**\n",
        "* Como os computadores calculam as chances de uma palavra ou sequência de palavras ocorrerem em um texto.\n",
        "* Utilização de um corpus de texto para calcular as probabilidades."
      ],
      "metadata": {
        "id": "u53zjZX_3rYG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuiCUGVruKHi"
      },
      "source": [
        "# Modelos de Linguagem com N-gramas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clRTlWOQuazy"
      },
      "source": [
        "Baixando a versão 3.5 do NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi00CCGWrdbM",
        "outputId": "bc084441-1bba-4a59-8573-698d0671d3ba"
      },
      "source": [
        "!pip install nltk==3.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (1.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (4.66.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434677 sha256=10fae1fabb5499cd00832a9ae5f2f15b4eba55365b76b0611051516300425083\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/ab/82/f9667f6f884d272670a15382599a9c753a1dfdc83f7412e37d\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRPRwUkNqYVe",
        "outputId": "8ad2363e-860f-4244-db38-97245f778f1a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF85PEK7umzZ"
      },
      "source": [
        "### Passo 1: Carregando o Córpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cxKQ5cEqd_0",
        "outputId": "d9859bae-3cee-4519-fd5f-38c6234745c5"
      },
      "source": [
        "texto = \"\"\"No meio do caminho tinha uma pedra\n",
        "Tinha uma pedra no meio do caminho\n",
        "Tinha uma pedra\n",
        "No meio do caminho tinha uma pedra\"\"\"\n",
        "\n",
        "texto = texto.lower().split('\\n')\n",
        "texto"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['no meio do caminho tinha uma pedra',\n",
              " 'tinha uma pedra no meio do caminho',\n",
              " 'tinha uma pedra',\n",
              " 'no meio do caminho tinha uma pedra']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGw-zxQRusNm"
      },
      "source": [
        "### Passo 2: Tokenizando as Sentenças do Córpus\n",
        "\n",
        "Segmentação e Padronização de Textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOubfzgWqo3a",
        "outputId": "d22195fd-e24f-4dc9-91d9-d52498f70dd9"
      },
      "source": [
        "texto_tok = []\n",
        "for verso in texto:\n",
        "  tokens = nltk.word_tokenize(verso, language='portuguese')\n",
        "  texto_tok.append(tokens)\n",
        "\n",
        "texto_tok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra'],\n",
              " ['tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho'],\n",
              " ['tinha', 'uma', 'pedra'],\n",
              " ['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s36lDZhzQge"
      },
      "source": [
        "## Pré-processando as Sentenças\n",
        "\n",
        "### Passo 3: Inserindo Marcadores de Início e Fim de Sentença\n",
        "Suponha que queiramos definir um modelo de linguagem com bigramas, ou seja, calcular as chances de uma palavra com base na anterior (e.g., $P(pedra | uma)$, temos que marcar o início e fim da sentença para poder prever as changes da primeira palavra (e.g., $P(no | \\langle s \\rangle)$) e o fim da sentença ((e.g., $P(\\langle/ s \\rangle)$ | pedra)). Este processo é conhecido como *padding*.\n",
        "\n",
        "Podemos fazer o *padding* de uma sentença utilizando o método **nltk.lm.preprocessing.pad_both_ends**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmdTktH72UDO",
        "outputId": "2d9a9b3a-bf88-4903-bcf8-ac09c0b8de5e"
      },
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "ngramas = 2 # definindo o número de n-gramas (no caso, 2 -> bigramas)\n",
        "\n",
        "texto_tok_pad = []\n",
        "for verso in texto_tok:\n",
        "  padded = pad_both_ends(verso, n=ngramas)\n",
        "  texto_tok_pad.append(list(padded))\n",
        "\n",
        "texto_tok_pad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<s>', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '</s>'],\n",
              " ['<s>', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', '</s>'],\n",
              " ['<s>', 'tinha', 'uma', 'pedra', '</s>'],\n",
              " ['<s>', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '</s>']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAfVzYnX3etA"
      },
      "source": [
        "### Passo 4: Calculando os N-Gramas\n",
        "\n",
        "Uma vez que as sentenças do nosso córpus foram pré-processadas, podemos calcular os n-gramas (neste caso, os bigramas), utilizando o método **nltk.ngrams**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVS0TTf04JJC",
        "outputId": "721e283c-e3d5-4a2a-f2dd-3451ed8bec34"
      },
      "source": [
        "ngramas = 2\n",
        "\n",
        "bigramas_pad = []\n",
        "for verso in texto_tok_pad:\n",
        "  bigramas = nltk.ngrams(verso, ngramas)\n",
        "  bigramas_pad.append(list(bigramas))\n",
        "\n",
        "bigramas_pad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')],\n",
              " [('<s>', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', '</s>')],\n",
              " [('<s>', 'tinha'), ('tinha', 'uma'), ('uma', 'pedra'), ('pedra', '</s>')],\n",
              " [('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33X7LWud6Max"
      },
      "source": [
        "Contudo, para deixar nosso modelo de linguagem mais robusto, vamos calcular os **unigramas** além dos **bigramas** utilizando o comando **nltk.util.everygrams**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtEqViQ75-C8",
        "outputId": "6df1a187-0d41-435f-9d1e-82e75e3f2da6"
      },
      "source": [
        "from nltk.util import everygrams\n",
        "ngramas = 2\n",
        "\n",
        "ngramas_pad = []\n",
        "for verso in texto_tok_pad:\n",
        "  bigramas = everygrams(verso, max_len=ngramas)\n",
        "  ngramas_pad.append(list(bigramas))\n",
        "\n",
        "ngramas_pad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('<s>',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')],\n",
              " [('<s>',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', '</s>')],\n",
              " [('<s>',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')],\n",
              " [('<s>',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QppWnvIPu-mP"
      },
      "source": [
        "### Passo 5: Colocando todos os tokens do córpus numa única lista\n",
        "**nltk.lm.preprocessing.flatten**:\n",
        "\n",
        "Este método converte junta os elementos de sublistas em uma única lista. Por exemplo:\n",
        "\n",
        "```python\n",
        ">>> lista = [[1, 2], [3, 4]]\n",
        ">>> flatten(lista)\n",
        "[1, 2, 3, 4]\n",
        "```\n",
        "\n",
        "Como pode ser visto abaixo, nós o utilizamos para juntar todas os tokens das sentenças de nosso corpus numa única lista."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCeQQUg2rIiC",
        "outputId": "55308482-0f1a-4095-fb12-55523eed30dc"
      },
      "source": [
        "from nltk.lm.preprocessing import flatten\n",
        "\n",
        "tokens = list(flatten(texto_tok_pad)) # juntando as palavras do nosso córpus\n",
        "\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'no',\n",
              " 'meio',\n",
              " 'do',\n",
              " 'caminho',\n",
              " 'tinha',\n",
              " 'uma',\n",
              " 'pedra',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'tinha',\n",
              " 'uma',\n",
              " 'pedra',\n",
              " 'no',\n",
              " 'meio',\n",
              " 'do',\n",
              " 'caminho',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'tinha',\n",
              " 'uma',\n",
              " 'pedra',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'no',\n",
              " 'meio',\n",
              " 'do',\n",
              " 'caminho',\n",
              " 'tinha',\n",
              " 'uma',\n",
              " 'pedra',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdP9qZo3LTb"
      },
      "source": [
        "### Passo 6: Definindo o Vocabulário\n",
        "\n",
        "**nltk.lm.Vocabulary**\n",
        "\n",
        "Utilizado para definir o vocabulário do nosso córpus. Recebe dois parâmetros como entrada: uma lista com todos os tokens do nosso córpus e a variável *unk_cutoff*, a qual passa a considerar palavras abaixo de um limiar de frequência como palavras fora do vocabuário."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOOurwmM3PYJ"
      },
      "source": [
        "from nltk.lm import Vocabulary\n",
        "\n",
        "vocab = Vocabulary(tokens, unk_cutoff=1) # definindo o vocabulário do nosso córpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4u_aGeSxQ6s"
      },
      "source": [
        "Obtendo as frequências das palavras do córpus com o comando *counts*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJnor-aUtCHr",
        "outputId": "6f79f776-eaeb-4cc5-b0b3-a47edf24c0d2"
      },
      "source": [
        "vocab.counts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'<s>': 4,\n",
              "         'no': 3,\n",
              "         'meio': 3,\n",
              "         'do': 3,\n",
              "         'caminho': 3,\n",
              "         'tinha': 4,\n",
              "         'uma': 4,\n",
              "         'pedra': 4,\n",
              "         '</s>': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFCYYcHW9J-_"
      },
      "source": [
        "procurando uma palavra no vocabulário. Caso não encontrada, o token de palavra fora do vocabulário será retornada (\\<UNK>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JslWfsYtKsF",
        "outputId": "e74cb4bc-a9ac-4245-9eb3-641ea83f605f"
      },
      "source": [
        "vocab.lookup(\"tinha\"), vocab.lookup(\"homem\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tinha', '<UNK>')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-YaGnSZ8SBn"
      },
      "source": [
        "## Simplificando o Pré-processamento\n",
        "\n",
        "Agora que você sabe cada passo do pré-processamento (inserir marcadores de início e fim de sentença, calcular os n-gramas, juntar todos os tokens do corpus numa lista e definir o vocabulário), este processo pode ser simplificado pela funcionalidade **nltk.lm.preprocessing.padded_everygram_pipeline**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ4w1KfL90UD",
        "outputId": "86137ca1-f12e-446e-de07-b02fc3b3a965"
      },
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "ngramas = 2\n",
        "\n",
        "ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)\n",
        "\n",
        "ngramas_pad = [list(w) for w in ngramas_pad]\n",
        "ngramas_pad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('<s>',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')],\n",
              " [('<s>',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', '</s>')],\n",
              " [('<s>',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')],\n",
              " [('<s>',),\n",
              "  ('no',),\n",
              "  ('meio',),\n",
              "  ('do',),\n",
              "  ('caminho',),\n",
              "  ('tinha',),\n",
              "  ('uma',),\n",
              "  ('pedra',),\n",
              "  ('</s>',),\n",
              "  ('<s>', 'no'),\n",
              "  ('no', 'meio'),\n",
              "  ('meio', 'do'),\n",
              "  ('do', 'caminho'),\n",
              "  ('caminho', 'tinha'),\n",
              "  ('tinha', 'uma'),\n",
              "  ('uma', 'pedra'),\n",
              "  ('pedra', '</s>')]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x79xHcBh-MSC"
      },
      "source": [
        "## Passo 7: Treinando um modelo de linguagem\n",
        "\n",
        "Um modelo de linguagem pode ser treinado utilizando a funcionalidade **nltk.lm.MLE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-BnVhfXq3G5"
      },
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline, flatten\n",
        "from nltk.lm import MLE\n",
        "\n",
        "ngramas = 2\n",
        "ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)\n",
        "lm = MLE(ngramas)\n",
        "lm.fit(ngramas_pad, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg0bzlALP7zq"
      },
      "source": [
        "Dado o token **\\<s>**, gerando um texto de 5 tokens com o modelo de linguagem treinado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJgyGcGAsciL",
        "outputId": "a2788036-7fce-4983-e06e-805d35510cba"
      },
      "source": [
        "lm.generate(5, text_seed=[\"<s>\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['no', 'meio', 'do', 'caminho', 'tinha']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoESEalNQPdv"
      },
      "source": [
        "Probabilidade da palavra *no*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icB93IpOsEnq",
        "outputId": "ad0d603d-4da3-4658-bc3c-143d2f76a700"
      },
      "source": [
        "print(\"Probabilidade comum: \", lm.score(\"no\"))\n",
        "print(\"Probabilidade logarítmica: \", lm.logscore(\"no\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidade comum:  0.09375\n",
            "Probabilidade logarítmica:  -3.415037499278844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrHPUljYQ3jg"
      },
      "source": [
        "Probabilidade da palavra *tinha* dado a palavra *caminho*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJNenPk_RBvG",
        "outputId": "a782cd83-f1a6-4513-f759-75988b8f001d"
      },
      "source": [
        "print(\"Probabilidade comum: \", lm.score(\"tinha\", context=[\"caminho\"]))\n",
        "print(\"Probabilidade logarítmica: \", lm.logscore(\"tinha\", context=[\"caminho\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidade comum:  0.6666666666666666\n",
            "Probabilidade logarítmica:  -0.5849625007211563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVSiSfdaomXD"
      },
      "source": [
        "## Avaliação Perplexidade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQCwukWynB1z",
        "outputId": "79cf23f1-6f74-416c-ecb4-d755040effb1"
      },
      "source": [
        "teste = \"\"\"Tinha uma pedra\n",
        "No meio do caminho\n",
        "Tinha uma pedra\"\"\"\n",
        "\n",
        "# pré-processamento\n",
        "teste = teste.lower().split('\\n')\n",
        "teste_tok = []\n",
        "for verso in teste:\n",
        "  tokens = nltk.word_tokenize(verso, language='portuguese')\n",
        "  teste_tok.append(tokens)\n",
        "\n",
        "ngramas = 1\n",
        "teste_ngramas, _ = padded_everygram_pipeline(ngramas, teste_tok)\n",
        "teste_ngramas = flatten([list(w) for w in teste_ngramas])\n",
        "print(\"Perplexidade do Unigrama: \", lm.perplexity(teste_ngramas))\n",
        "\n",
        "ngramas = 2\n",
        "teste_ngramas, _ = padded_everygram_pipeline(ngramas, teste_tok)\n",
        "teste_ngramas = flatten([list(w) for w in teste_ngramas])\n",
        "print(\"Perplexidade do Bigrama: \", lm.perplexity(teste_ngramas))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexidade do Unigrama:  8.975641163569597\n",
            "Perplexidade do Bigrama:  3.7299192471355798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW1od-7D9RBG"
      },
      "source": [
        "## Add-1 Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qcyqAuepcr6"
      },
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import Laplace\n",
        "\n",
        "ngramas = 2\n",
        "ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)\n",
        "lm = Laplace(ngramas)\n",
        "lm.fit(ngramas_pad, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05pNfw1l9T9l",
        "outputId": "1892d038-7625-4772-9f8b-3eccdc2160d8"
      },
      "source": [
        "round(lm.score(\"tinha\", context=[\"caminho\"]), 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCbM67FyFUUk"
      },
      "source": [
        "## Add-k Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St1C8Los9X4I"
      },
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import Lidstone\n",
        "\n",
        "ngramas = 2\n",
        "k=0.1\n",
        "ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)\n",
        "lm = Lidstone(order=ngramas, gamma=k)\n",
        "lm.fit(ngramas_pad, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag5hb9R8Fpew",
        "outputId": "1294178e-b742-42b5-d9bb-8b6dcf6a3018"
      },
      "source": [
        "round(lm.score(\"tinha\", context=[\"caminho\"]), 2) # dado a palavra 'caminho' a probabilidade da proxima palavra ser 'tinha', é de 0.53 de acordo com nosso corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Lidstone é um modelo de linguagem probabilístico que utiliza o k-smoothing. O k-smoothing adiciona k a cada conta de frequência observada."
      ],
      "metadata": {
        "id": "PQnihFQTV9Ij"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmcqFAQ_FsJO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}