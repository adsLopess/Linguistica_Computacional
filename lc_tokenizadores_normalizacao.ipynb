{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adsLopess/Linguistica_Computacional/blob/main/lc_tokenizadores_normalizacao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meyc7mPM2rDK"
      },
      "source": [
        "# Aula 3 - Normalização de Texto\n",
        "## DCC: Linguística Computacional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e_NtMSW2wQ_"
      },
      "source": [
        "## Tokenizadores\n",
        "\n",
        "Importando a biblioteca de expressões regulares do Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr-J_pGeCEQL"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI3GMSOv26F2"
      },
      "source": [
        "### Tokenizador por Espaço\n",
        "\n",
        "Desenvolvido com base no principal delimitador para uma grande parcela das línguas naturais humanas: o espaço"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBiqYr4kGlLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb6a697-f9cf-4167-d8c6-b9d4b966ea74"
      },
      "source": [
        "texto = \"No meio do caminho tinha uma pedra.\"\n",
        "\n",
        "texto.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufM9ES0_3FwF"
      },
      "source": [
        "### Tokenizador baseado numa expressão regular\n",
        "\n",
        "Segmenta as palavras de um texto com base em delimitadores como espaço, pontuações e início/fim de uma sequência (\\b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIllpjMcCIOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac802bb-3690-45ec-b7ef-f54bce542ec8"
      },
      "source": [
        "texto = \"No meio do caminho tinha uma pedra.\"\n",
        "\n",
        "re.sub(r\"(\\b)\", r\" \\1\", texto).split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['No', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwGtvVJt3Nni"
      },
      "source": [
        "### Tokenizador baseado em Regras\n",
        "\n",
        "1. Buscar todas as ocorrências de valores numéricos e financeiros (R$1,00; $46; etc.)\n",
        "\n",
        "2. Buscar todas as ocorrências de sequências de 1 ou mais caracteres\n",
        "\n",
        "3. Buscar todas as ocorrências de sequências sem espaço\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95ecY9vrFdph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45bfeb4c-f2e8-4af3-b518-300fc3f8d951"
      },
      "source": [
        "texto = \"Eu paguei R$456,00 pelo setup. O que acha?\"\n",
        "regex = r\"R?\\$?[\\d\\.\\,]+|\\w+|\\S+\"\n",
        "re.findall(regex, texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Eu', 'paguei', 'R$456,00', 'pelo', 'setup', '.', 'O', 'que', 'acha', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ne3-KmR3cvw"
      },
      "source": [
        "### Tokenizador baseado em Regras do NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mts346uy3rBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7295fdc6-007b-49c8-8011-dc1c3f7bf9e8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "versos = \"\"\"O menino jogou bola ontem às 16:00.\"\"\"\n",
        "\n",
        "nltk.word_tokenize(versos, language='english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'menino', 'jogou', 'bola', 'ontem', 'às', '16:00', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naQ43NoL37sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafe41b3-027f-463e-df40-580aeba85a7a"
      },
      "source": [
        "text = \"\"\"Hello everyone!!!\"\"\"\n",
        "\n",
        "nltk.word_tokenize(text, language='english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '!', '!', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOG1RSu0rL4L"
      },
      "source": [
        "### Byte-Pair Encoding (BPE)\n",
        "\n",
        "[Fonte](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html)\n",
        "\n",
        "Baixando as dependências"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxQIjNvjrtSj",
        "outputId": "a2e1eb77-0435-412c-cf95-1d7b7ab5b761"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANolu3Pi4OGy"
      },
      "source": [
        "Baixando córpus do Wikipedia para treinar o tokenizador\n",
        "\n",
        "[Fonte](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxQH9OlrrNCL",
        "outputId": "462f6316-9479-46e5-838c-623c8f7f44fc"
      },
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-07 16:29:27--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.227.96, 52.216.215.32, 16.182.98.32, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.227.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‘wikitext-103-raw-v1.zip’\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  46.3MB/s    in 4.2s    \n",
            "\n",
            "2024-02-07 16:29:32 (43.3 MB/s) - ‘wikitext-103-raw-v1.zip’ saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "   creating: wikitext-103-raw/\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FRrii3R46wI"
      },
      "source": [
        "Inicializando o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgmJn4L4rnBu"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1y30aQw48rw"
      },
      "source": [
        "Inicializando o Módulo de Treinamento\n",
        "\n",
        "Define-se um vocabulário desejado com 30000 símbolos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrRuKR2or9nt"
      },
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                     vocab_size=30000,\n",
        "                     min_frequency=0,\n",
        "                     continuing_subword_prefix=\"##\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfvB8y7P5HEB"
      },
      "source": [
        "Definindo pré-tokenizador por espaço"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6404tNdlsIQ_"
      },
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M8Rtpnu5KLr"
      },
      "source": [
        "Treinando o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lTS1iQCsPcq"
      },
      "source": [
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "tokenizer.train(files, trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFGUltIb5Lyy"
      },
      "source": [
        "Salvando o Tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRfnKqAatc1r"
      },
      "source": [
        "tokenizer.save(\"tokenizer-wiki.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoFlPUsO5ODO"
      },
      "source": [
        "Carregando o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCvQ32hwthws"
      },
      "source": [
        "tokenizer = Tokenizer.from_file(\"tokenizer-wiki.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oal7SnqX5QtB"
      },
      "source": [
        "Tokenizando textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSEpt2mptm40",
        "outputId": "9f6de279-50b4-4fcb-f732-e0cbd1a9d803"
      },
      "source": [
        "texto = \"I don't go out tonight.\"\n",
        "output = tokenizer.encode(texto)\n",
        "output.tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'don', \"'\", 't', 'go', 'out', 'ton', '##ight', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cIIRkbfuODC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "de475bb9-7d11-4ecd-d588-bb5f3151154a"
      },
      "source": [
        "tokenizer.decode(output.ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don ' t go out ton ##ight .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1RQXgSHzh-Z"
      },
      "source": [
        "### Byte-level BPE\n",
        "\n",
        "[Fonte](https://huggingface.co/blog/how-to-train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBARwePU5cLr"
      },
      "source": [
        "Inicializando o tokenizador e o córpus de treinamento (Wikipedia)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IldXH5svhyTA"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnK8k80H5ihF"
      },
      "source": [
        "Treinando o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfo_-1mfzyJX"
      },
      "source": [
        "tokenizer.train(files=files, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nsS_zQY5kl8"
      },
      "source": [
        "Salvando o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOYiuTD35o2I"
      },
      "source": [
        "tokenizer.save(\"tokenizer-wiki.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxfXNBh15rVB"
      },
      "source": [
        "Tokenizando um texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZrrTGqmzy0N",
        "outputId": "8ce16574-90fb-4326-a9dc-1f56964a9884"
      },
      "source": [
        "output = tokenizer.encode(\"Eu estou na aula de Línguística Computacional.\")\n",
        "output.tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['E',\n",
              " 'u',\n",
              " 'Ġest',\n",
              " 'ou',\n",
              " 'Ġna',\n",
              " 'Ġa',\n",
              " 'ula',\n",
              " 'Ġde',\n",
              " 'ĠL',\n",
              " 'ÃŃn',\n",
              " 'gu',\n",
              " 'ÃŃ',\n",
              " 'st',\n",
              " 'ica',\n",
              " 'ĠComp',\n",
              " 'ut',\n",
              " 'ac',\n",
              " 'ional',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjI8d9wt5yK3"
      },
      "source": [
        "### WordPiece\n",
        "\n",
        "Inicializando o Tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SctmNraF0q0K"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "\n",
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_YVqexC60Gv"
      },
      "source": [
        "Inicializando o módulo de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLnxKAzw6pBU"
      },
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                     vocab_size=30000,\n",
        "                     min_frequency=0,\n",
        "                     continuing_subword_prefix=\"##\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ERuAvdB7f7E"
      },
      "source": [
        "Inicializando pré-tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjxZrjAY61UJ"
      },
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y28phCU7mMc"
      },
      "source": [
        "Treinando o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahVzjV747lvV"
      },
      "source": [
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "tokenizer.train(files, trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2B0LADX7qLh"
      },
      "source": [
        "Tokenizando o texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF4sz4dp7pgt",
        "outputId": "8f75ac64-e0bb-49df-884e-d7f00f991f9a"
      },
      "source": [
        "output = tokenizer.encode(\"Eu estou na aula.\")\n",
        "output.tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Eu', 'est', '##ou', 'na', 'a', '##ula', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8jS3eke8M2R"
      },
      "source": [
        "### Unigram\n",
        "\n",
        "Semelhante aos outros modelos. Pode precisar de GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IagREqtF8GIB"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import Unigram\n",
        "from tokenizers.trainers import UnigramTrainer\n",
        "\n",
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "\n",
        "# inicializando o tokenizador\n",
        "tokenizer = Tokenizer(Unigram())\n",
        "# inicilizando o módulo de treinamento\n",
        "trainer = UnigramTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                     vocab_size=30000,\n",
        "                     min_frequency=0,\n",
        "                     continuing_subword_prefix=\"##\")\n",
        "# treinando o tokenizador\n",
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "tokenizer.train(files, trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GST-vvJG8m_w"
      },
      "source": [
        "output = tokenizer.encode(\"I will play games tonight.\")\n",
        "output.tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDD7hanr_SMK"
      },
      "source": [
        "## Capitalização\n",
        "\n",
        "Processo de colocar os tokens em letra minúscula para normalização do texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYp6P1WX_Vhw"
      },
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "versos = \"\"\"No meio do caminho tinha uma pedra\n",
        "Tinha uma pedra no meio do caminho\"\"\".lower()\n",
        "\n",
        "nltk.word_tokenize(versos, language='portuguese')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foSgAc2F5ghm"
      },
      "source": [
        "### Tokenização de Sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge8X2VbI5kVI",
        "outputId": "0466ee91-4a06-4e01-9359-027dc19b00b2"
      },
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "texto = 'Eu estou na aula de Linguística Computacional. Os estudantes são muito bons.'\n",
        "\n",
        "nltk.sent_tokenize(texto, language='portuguese')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Eu estou na aula de Linguística Computacional.',\n",
              " 'Os estudantes são muito bons.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmxVHYpG_qE0"
      },
      "source": [
        "## Lematização\n",
        "\n",
        "Instalando o Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdQ6onQU_gwW"
      },
      "source": [
        "!pip3 install -U pip setuptools wheel\n",
        "!pip3 install -U spacy[cuda102]==3\n",
        "!python3 -m spacy download pt_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fGJ1EsUAz_v"
      },
      "source": [
        "import spacy\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"pt_core_news_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X00D34c8A52e",
        "outputId": "0c7fc961-0a2a-41cc-cc34-d6a64472dab0"
      },
      "source": [
        "doc = nlp(\"O passado é só uma história que nos contamos.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O O\n",
            "passado passar\n",
            "é ser\n",
            "só só\n",
            "uma umar\n",
            "história história\n",
            "que que\n",
            "nos o\n",
            "contamos contar\n",
            ". .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0URMEF8-COdm"
      },
      "source": [
        "## Radicalização\n",
        "\n",
        "Identifica e remove prefixos e sufixos, buscando a raiz da palavra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxsabihJCMr3",
        "outputId": "004d2b6c-641b-4655-ac30-cb0734173886"
      },
      "source": [
        "import nltk\n",
        "# nltk.download('rslp')\n",
        "\n",
        "raiz = nltk.stem.RSLPStemmer()\n",
        "\n",
        "tokens = nltk.word_tokenize('A comida estava gostosa', language='portuguese')\n",
        "[raiz.stem(token) for token in tokens]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'com', 'est', 'gost']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK-Bq6cYDEuo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}